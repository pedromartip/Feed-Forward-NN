{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split):\n",
    "\n",
    "    # Load train data\n",
    "    data = read_csv('ds1-06-%d-nn-tr.csv' % split)\n",
    "    X_train = data.iloc[:, :-1].to_numpy()\n",
    "    y_train = data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Load test data\n",
    "    data = read_csv('ds1-06-%d-nn-te.csv' % split)\n",
    "    X_test = data.iloc[:, :-1].to_numpy()\n",
    "    y_test = data.iloc[:, -1].to_numpy()\n",
    "    \n",
    "    # One-hot encoding\n",
    "    y_train_OHE_OHE = to_categorical(y_train, num_classes=5)\n",
    "    y_test_OHE_OHE = to_categorical(y_test, num_classes=5)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, y_train_OHE_OHE, y_test_OHE_OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many output neurons as categories --> use softmax and train with (pg44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer_neurons, activation_function, learning_rate=0.1, decay_steps=1000, decay_rate=0.96, loss_function='categorical_crossentropy', output_neurons=5, dynamic = False, optimizer = tf.keras.optimizers.SGD):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Add hidden layers with specified number of neurons and activation function\n",
    "    for neurons in layer_neurons:\n",
    "        model.add(tf.keras.layers.Dense(neurons, activation=activation_function))\n",
    "    \n",
    "    if dynamic == True:\n",
    "        lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=learning_rate,\n",
    "            decay_steps=decay_steps,\n",
    "            decay_rate=decay_rate)\n",
    "    else: # Para primeras tareas\n",
    "        lr = learning_rate\n",
    "\n",
    "    # Output layer with softmax activation for multi-class classification\n",
    "    model.add(tf.keras.layers.Dense(output_neurons, activation='softmax'))\n",
    "\n",
    "    # Compile the model with the specified optimizer and loss function\n",
    "    model.compile(optimizer =  optimizer(learning_rate=lr), \n",
    "                  loss = loss_function, \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Start with a basic configuration: a single hidden layer network and the most basic activation function. Try with three different amounts of neurons, and keep the best configuration for the next task.\n",
    "### For training, set a fixed learning rate, the most basic optimizer, a reasonable batch size, the most direct loss function and choose enough epochs to let the training converge. Tune the learning rate until you achieve convergence and a reasonable performance. To measure performance, use the accuracy for the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have a look if our classes are enconded as integers or one-hot encoded, in order to use the appropiated `loss_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_train_OHE, y_test_OHE = load_data(split=1)\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to create a a one-layer network and try it on a different amount of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons: 10, Test Accuracy (median): 0.9101796388626099\n",
      "Neurons: 50, Test Accuracy (median): 0.9233533143997192\n",
      "Neurons: 100, Test Accuracy (median): 0.9329341292381287\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train_1, y_train_OHE, X_test, y_test_OHE = load_data(1)\n",
    "\n",
    "neurons_amount = [10,50,100]\n",
    "\n",
    "for neurons in neurons_amount:\n",
    "    promed = []\n",
    "    model = create_model(neurons,  activation_function='relu')\n",
    "    for i in range(5):\n",
    "        model.fit(X_train, y_train_OHE, epochs = 50, batch_size = 32, verbose=0) # Quitar verbose para ver cada epoch\n",
    "        performance = model.evaluate(X_test, y_test_OHE, verbose=0)\n",
    "        promed.append(performance[1])\n",
    "    print(f\"Neurons: {neurons}, Test Accuracy (median): {np.mean(promed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we used the `relu` activation function and the `sparse_categorical_crossentropy` as our lose function, due to we are facing a .........!!!! becauseAs it can see, the best performance is in the biggest amount, having an accuracy of 79%, bigger than the other amounts of neurons. So, we consider to take that amount of `100` neurons as the best configuration and test it for the following task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Next, check whether a change in the activation function of the hidden layer neurons improves the classification performance. If that is the case, continue with the alternative activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test if a change in the activation functions can work better then the previous one in T1, we are going to test it in a different amount of activation functions. With that, we will test the most used activation functions and we will be able to compare a reasonable amount of activation functions in order to choose the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons: 100, Activation: relu, Test Accuracy: 0.9221556782722473\n",
      "Neurons: 100, Activation: sigmoid, Test Accuracy: 0.7784430980682373\n",
      "Neurons: 100, Activation: tanh, Test Accuracy: 0.8982036113739014\n",
      "Neurons: 100, Activation: leaky_relu, Test Accuracy: 0.8982036113739014\n",
      "Neurons: 100, Activation: elu, Test Accuracy: 0.8622754216194153\n",
      "Neurons: 100, Activation: selu, Test Accuracy: 0.8982036113739014\n"
     ]
    }
   ],
   "source": [
    "activation_functions = ['relu','sigmoid', 'tanh', 'leaky_relu', 'elu', 'selu'] \n",
    "\n",
    "for activation in activation_functions:\n",
    "    model = create_model(100, activation)\n",
    "    model.fit(X_train, y_train_OHE, epochs=50, batch_size=32, verbose=0)\n",
    "    performance = model.evaluate(X_test, y_test_OHE, verbose=0)\n",
    "    print(f\"Neurons: {neurons}, Activation: {activation}, Test Accuracy: {performance[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is shown, the best performance of the activations is the `relu` activation, reaching a 92% of accuracy. Having a good puntuation but still smaller than our prior configuration, that wins in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T3. Try with a dynamic learning rate and use it from now on if the performance does not get worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dynamic learning rate\n",
      "Accuracy using Dynamic Learning Rate: : 0.9191616773605347\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    neurons=100,\n",
    "    activation_function='relu',\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    dynamic = True\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train_OHE, epochs=50, batch_size=32, verbose=0)\n",
    "performance = model.evaluate(X_test, y_test_OHE, verbose=0)\n",
    "print(f\"Accuracy using Dynamic Learning Rate: : {performance[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4. Change to an alternative optimizer and keep it if the performance gets better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dynamic learning rate\n",
      "Accuracy using <class 'keras.src.optimizers.sgd.SGD'>: 0.9281437397003174\n",
      "Using dynamic learning rate\n",
      "Accuracy using <class 'keras.src.optimizers.adam.Adam'>: 0.7934131622314453\n",
      "Using dynamic learning rate\n",
      "Accuracy using <class 'keras.src.optimizers.adadelta.Adadelta'>: 0.8233532905578613\n",
      "Using dynamic learning rate\n",
      "Accuracy using <class 'keras.src.optimizers.rmsprop.RMSprop'>: 0.8443113565444946\n"
     ]
    }
   ],
   "source": [
    "optimizers = [tf.keras.optimizers.SGD,\n",
    "              tf.keras.optimizers.Adam, \n",
    "              tf.keras.optimizers.Adadelta, \n",
    "              tf.keras.optimizers.RMSprop]\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    model = create_model(\n",
    "        layer_neurons=[100],\n",
    "        activation_function='relu',\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        dynamic = True,\n",
    "        optimizer= optimizer\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train_OHE, epochs=50, batch_size=32, verbose=0)\n",
    "    performance = model.evaluate(X_test, y_test_OHE, verbose=0)\n",
    "    print(f\"Accuracy using {optimizer}: {performance[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5. Switch to another loss function to check whether the performance level increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using categorical_crossentropy: 0.9131736755371094\n",
      "Accuracy using sparse_categorical_crossentropy: 0.910179615020752\n",
      "Accuracy using Poisson: 0.826347291469574\n",
      "Accuracy using KLDivergence: 0.9221556782722473\n",
      "Accuracy using mean_squared_error: 0.817365288734436\n"
     ]
    }
   ],
   "source": [
    "loss_functions = ['categorical_crossentropy','sparse_categorical_crossentropy','Poisson', 'KLDivergence','mean_squared_error']\n",
    "neurons = [100]\n",
    "for loss_function in loss_functions:\n",
    "    model = create_model(\n",
    "        layer_neurons=neurons,\n",
    "        activation_function='relu',\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        dynamic = True,\n",
    "        optimizer=tf.keras.optimizers.SGD, # Como resultado anterior, seguimos con este\n",
    "        loss_function=loss_function\n",
    "    )\n",
    "    if('sparse_categorical_crossentropy' == loss_function): \n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "        performance = model.evaluate(X_test, y_test, verbose=0)\n",
    "    else:\n",
    "        model.fit(X_train, y_train_OHE, epochs=50, batch_size=32, verbose=0)\n",
    "        performance = model.evaluate(X_test, y_test_OHE, verbose=0)\n",
    "    print(f\"Accuracy using {loss_function}: {performance[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T6. Add a second hidden layer with a reasonable number of neurons and check whether a performance gain is obtained. If that is the case, keep the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using 100 neurons for first layer and 50 for second layer: 0.9371257424354553\n",
      "Accuracy using 100 neurons for first layer and 100 for second layer: 0.92514967918396\n",
      "Accuracy using 100 neurons for first layer and 150 for second layer: 0.9341317415237427\n"
     ]
    }
   ],
   "source": [
    "neurons = [[100,50],[100,100], [100,150]]\n",
    "for neuron in neurons:\n",
    "    model = create_model(\n",
    "        layer_neurons=neuron,\n",
    "        activation_function='relu',\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        dynamic = True,\n",
    "        optimizer=tf.keras.optimizers.SGD, # Como resultado anterior, seguimos con este\n",
    "        loss_function='categorical_crossentropy'\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train_OHE, epochs=50, batch_size=32, verbose=0)\n",
    "    performance = model.evaluate(X_test, y_test_OHE, verbose=0)\n",
    "    print(f\"Accuracy using {neuron[0]} neurons for first layer and {neuron[1]} for second layer: {performance[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T7. Try with larger and smaller batch sizes (one of each). You should observe that the training time also changes. Adopt the size leading to highest performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using batch of 2: 0.946107804775238. Execution time = 9.706257343292236\n",
      "Accuracy using batch of 8: 0.9491018056869507. Execution time = 2.8214809894561768\n",
      "Accuracy using batch of 16: 0.9281437397003174. Execution time = 1.6462347507476807\n",
      "Accuracy using batch of 32: 0.9281437397003174. Execution time = 1.065023422241211\n",
      "Accuracy using batch of 54: 0.8772454857826233. Execution time = 0.8220798969268799\n",
      "Accuracy using batch of 128: 0.8832335472106934. Execution time = 0.647169828414917\n"
     ]
    }
   ],
   "source": [
    "batchs = [2,8,16,32,54,128]\n",
    "for batch in batchs:\n",
    "    model = create_model(\n",
    "        layer_neurons=[100,150],\n",
    "        activation_function='relu',\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        dynamic = True,\n",
    "        optimizer=tf.keras.optimizers.SGD, # Como resultado anterior, seguimos con este\n",
    "        loss_function='categorical_crossentropy'\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train_OHE, epochs=50, batch_size=batch, verbose=0)\n",
    "    performance = model.evaluate(X_test, y_test_OHE, verbose=0)\n",
    "    print(f\"Accuracy using batch of {batch}: {performance[1]}. Execution time = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T8. For the best configuration found, determine the performance of the network using the accuracy, precision, recall and f1 metrics[2]. To this purpose, you have to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Train the network for every one of the splits (using the corresponding training set). Show that the training has achieved convergence in each case by means of an appropriate plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIV_P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
